{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af3d38",
   "metadata": {},
   "outputs": [],
   "execution_count": 1,
   "id": "a2af3d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CONC_MCPServer.src:Main package initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders import (\n",
    "    WebBaseLoader, \n",
    "    PyPDFLoader, \n",
    "    Docx2txtLoader,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "\n",
    "from src.core.config import get_settings\n",
    "\n",
    "settings = get_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536aa801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs\n",
    "from pathlib import Path\n",
    "\n",
    "root_path = Path(os.getcwd()).joinpath(\"assets\")\n",
    "doc_paths = [\n",
    "    root_path.joinpath(\"docs/test_rag.pdf\"),\n",
    "    root_path.joinpath(\"docs/test_rag.docx\"),\n",
    "]\n",
    "\n",
    "docs = [] \n",
    "for file_path in doc_paths:\n",
    "\n",
    "    try:\n",
    "        if file_path.suffix == \".pdf\":\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.suffix == \".docx\":\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_path.suffix == \".txt\" or file_path.suffix == \".md\":\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Document type {file_path.suffix} not supported.\")\n",
    "            continue\n",
    "\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {file_path.name}: {e}\")\n",
    "\n",
    "\n",
    "# Load URLs\n",
    "\n",
    "url = \"https://docs.streamlit.io/develop/quick-reference/release-notes\"\n",
    "try:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading document from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b847ecb",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcad8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=1000,\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accce46",
   "metadata": {},
   "source": [
    "## Tokenize and load the documents to the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and load the documents to the vector store\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f81544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "\n",
    "def _get_context_retriever_chain(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get inforamtion relevant to the conversation, focusing on the most recent messages.\"),\n",
    "    ])\n",
    "    retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "    return retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36996849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_rag_chain(llm):\n",
    "    retriever_chain = _get_context_retriever_chain(vector_db, llm)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "        \"\"\"You are a helpful assistant. You will have to answer to user's queries.\n",
    "        You will have some context to help with your answers, but now always would be completely related or helpful.\n",
    "        You can also use your knowledge to assist answering the user's queries.\\n\n",
    "        {context}\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    return create_retrieval_chain(retriever_chain, stuff_documents_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Generation\n",
    "\n",
    "llm_stream_openai = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # Here you could use \"o1-preview\" or \"o1-mini\" if you already have access to them\n",
    "    temperature=0.3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "llm_stream_anthropic = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    temperature=0.3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "llm_stream = llm_stream_openai  # Select between OpenAI and Anthropic models for the response\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the latest version of Streamlit?\"},\n",
    "]\n",
    "messages = [HumanMessage(content=m[\"content\"]) if m[\"role\"] == \"user\" else AIMessage(content=m[\"content\"]) for m in messages]\n",
    "\n",
    "conversation_rag_chain = get_conversational_rag_chain(llm_stream)\n",
    "response_message = \"*(RAG Response)*\\n\"\n",
    "for chunk in conversation_rag_chain.pick(\"answer\").stream({\"messages\": messages[:-1], \"input\": messages[-1].content}):\n",
    "    response_message += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e551983",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = settings.LYTIX_JUNIOR_FOUNDRY_URL\n",
    "model = settings.LYTIX_JUNIOR_FOUNDRY_MODEL\n",
    "model_version = settings.LYTIX_JUNIOR_FOUNDRY_MODEL_VERSION\n",
    "print(\"\\n\\n\")\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"MODEL: {model}\")\n",
    "print(f\"MODEL VERSION: {model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm_stream_new = AzureChatOpenAI(\n",
    "    azure_endpoint=url,\n",
    "    openai_api_version=model_version,\n",
    "    model_name=model,\n",
    "    openai_api_key=key,\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0.3,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "prompt = \"Tell me something about Azure\"\n",
    "\n",
    "for chunk in llm_stream_new.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bi-lytix-junior-azure-demo-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
